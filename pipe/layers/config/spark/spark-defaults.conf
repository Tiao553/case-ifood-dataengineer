# ---------------------------------------
# Core Cluster Configuration
# ---------------------------------------
spark.master=spark://spark-master:7077

# JARs necessários para integração com S3
#spark.jars=/opt/bitnami/spark/extjars/hadoop-aws-3.3.4.jar,/opt/bitnami/spark/extjars/aws-java-sdk-bundle-1.12.426.jar

# Memória de cada executor
spark.executor.memory=2560m
spark.executor.memoryOverhead=512m
spark.executor.cores=1

# logs configuration
spark.eventLog.enabled=true
spark.eventLog.dir=file:///opt/bitnami/spark/logs/events
spark.history.fs.logDirectory=file:///opt/bitnami/spark/logs/events
spark.eventLog.compress=true


# ---------------------------------------
# Particionamento e Escrita
# ---------------------------------------
# Tamanho máximo de partição ao ler arquivos
spark.sql.files.maxPartitionBytes=64m
spark.sql.files.openCostInBytes=4m

# Tamanho máximo de registros por arquivo ao escrever
spark.sql.files.maxRecordsPerFile=500000

# Número de partições para operações de shuffle
spark.sql.shuffle.partitions=200

# ---------------------------------------
# Adaptive Query Execution (AQE)
# ---------------------------------------
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.coalescePartitions.initialPartitionNum=200
spark.sql.adaptive.coalescePartitions.minPartitionSize=64m
spark.sql.adaptive.skewJoin.enabled=true

# ---------------------------------------
# S3A Configuration
# ---------------------------------------
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.endpoint=s3.amazonaws.com
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.access.key=
spark.hadoop.fs.s3a.secret.key=

# I/O e paralelismo
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.threads.max=200
spark.hadoop.fs.s3a.threads.core=50
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.buffer=disk
spark.hadoop.fs.s3a.buffer.dir=/tmp
spark.hadoop.fs.s3a.connection.timeout=100000
spark.hadoop.fs.s3a.attempts.maximum=3

# ---------------------------------------
# Resiliência e Heartbeats
# ---------------------------------------

spark.network.timeout=300s
spark.executor.heartbeatInterval=60s

